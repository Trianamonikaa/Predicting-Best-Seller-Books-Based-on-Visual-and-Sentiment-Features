{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_score,cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from contractions import CONTRACTIONS_DICT \n",
    "from Convert_Negation import CONVERT_NEGATION\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NER(review):\n",
    "    for i in range(len(review)):\n",
    "        text = review.Review.iloc[i]\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                if hasattr(chunk, 'label') and chunk.label:\n",
    "                    if chunk.label() == 'ORGANIZATION' or  chunk.label() == 'PERSON' or  chunk.label() == 'DATE' or  chunk.label() == 'LOCATION':\n",
    "                        name_value = ' '.join(child[0] for child in chunk.leaves())\n",
    "                        text = text.replace(name_value, \"\")\n",
    "                        review.Review.iloc[i] = text\n",
    "    return review\n",
    "\n",
    "def case_folding(review):\n",
    "    for i in range(len(review)):\n",
    "        text = review.Review[i].lower()\n",
    "        review.Review.iloc[i] = text \n",
    "    return review\n",
    "\n",
    "def expand_contraction(review):\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(CONTRACTIONS_DICT.keys()))\n",
    "    d = {}\n",
    "    index=0\n",
    "    for i in review.Review:\n",
    "        text = i\n",
    "        def replace(match):\n",
    "            return CONTRACTIONS_DICT[match.group(0)]\n",
    "        text = contractions_re.sub(replace, text)\n",
    "        #review = review.replace(text,\"\")\n",
    "        review.Review[index] = text\n",
    "        index+=1\n",
    "    return review\n",
    "\n",
    "def convert_negation(review):\n",
    "    convertNegation_re = re.compile('(%s)' % '|'.join(CONVERT_NEGATION.keys()))\n",
    "    d = {}\n",
    "    index=0\n",
    "    for i in review.Review:\n",
    "        text = i\n",
    "        def replace(match):\n",
    "            return CONVERT_NEGATION[match.group(0)]\n",
    "        text = convertNegation_re.sub(replace, text)\n",
    "        review.Review[index] = text\n",
    "        index+=1\n",
    "    return review\n",
    "\n",
    "def remove_punctuation(review):\n",
    "    remove = string.punctuation\n",
    "    for i in range(len(review)):\n",
    "        for kata in remove:\n",
    "            text = review.Review[i].replace(kata,\"\")\n",
    "            review.Review.iloc[i] = text \n",
    "    return review\n",
    "\n",
    "def stop_removal(review):\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    cachedStopWords = set(stopwords.words(\"english\"))\n",
    "    for i in range(len(review)):\n",
    "        text = review.Review.iloc[i]\n",
    "        teks =\" \".join([word for word in text.split() if word not in cachedStopWords])\n",
    "        review.Review.iloc[i] = teks\n",
    "    return review\n",
    "\n",
    "def stemming(review):\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(review)):\n",
    "        text = review.Review.iloc[i]\n",
    "        text = ps.stem(text)\n",
    "        review.Review.iloc[i] = text\n",
    "    return review\n",
    "\n",
    "def lemmatization (review):\n",
    "    lm = WordNetLemmatizer()\n",
    "    for i in range(len(review)):\n",
    "        text = review.Review.iloc[i]\n",
    "        text = lm.lemmatize(text)\n",
    "        review.Review.iloc[i] = text\n",
    "    return review\n",
    "\n",
    "def preprocessing_data(review):\n",
    "    hasil_ner = NER(review)\n",
    "    hasil_case_folding = case_folding(hasil_ner)\n",
    "    hasil_convert_negation = convert_negation(hasil_case_folding)\n",
    "    hasil_expand = expand_contraction(hasil_convert_negation)\n",
    "    hasil_remove_punctuation = remove_punctuation(hasil_expand)\n",
    "    hasil_stop_removal = stop_removal(hasil_remove_punctuation)\n",
    "    hasil_stemming = stemming( hasil_stop_removal)\n",
    "    hasil_lemmatization = lemmatization(hasil_stemming)\n",
    "    return hasil_lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_sentimen 4.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-257c6da3be5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m#### load SVM model for klasifikasi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'model_sentimen 4.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;31m# ##########################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#### load SVM feature for klasifikasi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_sentimen 4.pkl'"
     ]
    }
   ],
   "source": [
    "#### load SVM model for klasifikasi\n",
    "filename1= 'model_sentimen 4.pkl'\n",
    "loaded_model = pickle.load(open(filename1, 'rb'))\n",
    "# ##########################################################################################################\n",
    "#### load SVM feature for klasifikasi\n",
    "filename1='feature.pkl'\n",
    "loaded_feature = pickle.load(open(filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_review_buku = pd.read_excel('E:/_KEPERLUAN FINAL TA/TASI-14_REVISI_FIKS/data_review/Data_review_buku.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buku</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I was disgusted by the authorâ€™s shameless atte...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I am familiar with this tragedy. I found this ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>It is very sad to see the authors of this book...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Who would write this garbage</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>I HIGHLY doubt they would have slit anyone's ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Very sad they just wanted to make a quick buck.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>Do not buy or read this trash. My father was a...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>This is just disrespectful to our soldiers.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>If you only read one book this year, this is t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>I beg you...get your hands on a copy of this book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>If you enjoy WWII history, this is a fascinati...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>Probably one of the best books I have ever rea...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>You cannot read this one without becoming emot...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>You can feel what those men felt and it's diff...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>I enjoyed it, but it wasn't the most captivati...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Life is struggle and you need to fight every d...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>this version has type so small it is difficult...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5</td>\n",
       "      <td>i didn't really feel like the book changed my ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>Small font. Was surprised and disappointed to ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>I do not recommend for anyone to purchase.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>Not sure if I completely buy into the theories...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>The book is not an easy read but good for prov...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>I was very disappointed. it should come with a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>Boring and drags</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>Worth reading till end of camp life</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>I got lost in the theory after story told</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>The book is not the regular size</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>Beginning story was incredible. But the entire...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>this book is more like narrative of a man's st...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>A good book but thought I'd get more out of it...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>695</td>\n",
       "      <td>This is an inspiring book. I first heard of Bo...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>696</td>\n",
       "      <td>The book immediately captured my interest beca...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>696</td>\n",
       "      <td>This is a great book. It was recommended by a ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>696</td>\n",
       "      <td>I really enjoyed reading how the author was ab...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>696</td>\n",
       "      <td>This is a Jesus lover book. Misrepresented.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>697</td>\n",
       "      <td>Would not have bought the book at all but my b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>697</td>\n",
       "      <td>I was very disappointed in this book</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>697</td>\n",
       "      <td>This book was crazy redundant. I was not a fan...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>697</td>\n",
       "      <td>Terrible book!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>697</td>\n",
       "      <td>If you want to know how to lead successfully,...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>697</td>\n",
       "      <td>OKRs are designed to simply and efficiently ca...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>698</td>\n",
       "      <td>That will tell you if this book is worth it.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>698</td>\n",
       "      <td>Great read, it is applicable to everything we do!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>698</td>\n",
       "      <td>Considering how many successful companies and ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>698</td>\n",
       "      <td>Save your money and google OKR. This book shou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>698</td>\n",
       "      <td>Beware: This does not sync with the Audible ve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>699</td>\n",
       "      <td>This is the worst book I've ever purchased.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>699</td>\n",
       "      <td>A real knee slapper this book is. Disgusting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>699</td>\n",
       "      <td>This book is a wonderful, easily digestible s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>699</td>\n",
       "      <td>Really doubt people in a hurry can understand...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>699</td>\n",
       "      <td>Neil Tyson's explanations can easily entice on...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>699</td>\n",
       "      <td>I intend to look for another book by Neil deG...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>700</td>\n",
       "      <td>Would recommend for anyone that is into astron...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>700</td>\n",
       "      <td>The text was disjointed and rambling. He would...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>700</td>\n",
       "      <td>Highly disappointing!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>700</td>\n",
       "      <td>Bought this in hopes of educating myself effic...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>700</td>\n",
       "      <td>Lame book that only gives generic overviews an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>700</td>\n",
       "      <td>I did not enjoy reading this book. I expected ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>700</td>\n",
       "      <td>l started marking all of the recipes l wanted ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>700</td>\n",
       "      <td>Highly recommend this W30 cookbook!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Non Best Seller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2454 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Buku                                             Review  Rating  \\\n",
       "0        1  I was disgusted by the authorâ€™s shameless atte...     2.0   \n",
       "1        1  I am familiar with this tragedy. I found this ...     1.0   \n",
       "2        1  It is very sad to see the authors of this book...     1.0   \n",
       "3        2                       Who would write this garbage     1.0   \n",
       "4        2  Â I HIGHLY doubt they would have slit anyone's ...     1.0   \n",
       "5        2    Very sad they just wanted to make a quick buck.     1.0   \n",
       "6        2  Do not buy or read this trash. My father was a...     1.0   \n",
       "7        2        This is just disrespectful to our soldiers.     1.0   \n",
       "8        3  If you only read one book this year, this is t...     5.0   \n",
       "9        3  I beg you...get your hands on a copy of this book     5.0   \n",
       "10       3  If you enjoy WWII history, this is a fascinati...     5.0   \n",
       "11       3  Probably one of the best books I have ever rea...     5.0   \n",
       "12       3  You cannot read this one without becoming emot...     5.0   \n",
       "13       4  You can feel what those men felt and it's diff...     5.0   \n",
       "14       4  I enjoyed it, but it wasn't the most captivati...     3.0   \n",
       "15       4  Life is struggle and you need to fight every d...     3.0   \n",
       "16       4  this version has type so small it is difficult...     3.0   \n",
       "17       5  i didn't really feel like the book changed my ...     3.0   \n",
       "18       5  Small font. Was surprised and disappointed to ...     2.0   \n",
       "19       5         I do not recommend for anyone to purchase.     2.0   \n",
       "20       6  Not sure if I completely buy into the theories...     3.0   \n",
       "21       6  The book is not an easy read but good for prov...     3.0   \n",
       "22       6     I was very disappointed. it should come with a     2.0   \n",
       "23       6                                   Boring and drags     2.0   \n",
       "24       7                Worth reading till end of camp life     3.0   \n",
       "25       7          I got lost in the theory after story told     3.0   \n",
       "26       7                   The book is not the regular size     2.0   \n",
       "27       8  Beginning story was incredible. But the entire...     3.0   \n",
       "28       8  this book is more like narrative of a man's st...     3.0   \n",
       "29       8  A good book but thought I'd get more out of it...     2.0   \n",
       "...    ...                                                ...     ...   \n",
       "2424   695  This is an inspiring book. I first heard of Bo...     5.0   \n",
       "2425   696  The book immediately captured my interest beca...     5.0   \n",
       "2426   696  This is a great book. It was recommended by a ...     5.0   \n",
       "2427   696  I really enjoyed reading how the author was ab...     5.0   \n",
       "2428   696        This is a Jesus lover book. Misrepresented.     1.0   \n",
       "2429   697  Would not have bought the book at all but my b...     1.0   \n",
       "2430   697               I was very disappointed in this book     1.0   \n",
       "2431   697  This book was crazy redundant. I was not a fan...     1.0   \n",
       "2432   697                                     Terrible book!     1.0   \n",
       "2433   697   If you want to know how to lead successfully,...     5.0   \n",
       "2434   697  OKRs are designed to simply and efficiently ca...     5.0   \n",
       "2435   698       That will tell you if this book is worth it.     5.0   \n",
       "2436   698  Great read, it is applicable to everything we do!     5.0   \n",
       "2437   698  Considering how many successful companies and ...     5.0   \n",
       "2438   698  Save your money and google OKR. This book shou...     1.0   \n",
       "2439   698  Beware: This does not sync with the Audible ve...     1.0   \n",
       "2440   699        This is the worst book I've ever purchased.     1.0   \n",
       "2441   699       A real knee slapper this book is. Disgusting     1.0   \n",
       "2442   699   This book is a wonderful, easily digestible s...     5.0   \n",
       "2443   699   Really doubt people in a hurry can understand...     5.0   \n",
       "2444   699  Neil Tyson's explanations can easily entice on...     5.0   \n",
       "2445   699   I intend to look for another book by Neil deG...     5.0   \n",
       "2446   700  Would recommend for anyone that is into astron...     5.0   \n",
       "2447   700  The text was disjointed and rambling. He would...     1.0   \n",
       "2448   700                              Highly disappointing!     1.0   \n",
       "2449   700  Bought this in hopes of educating myself effic...     1.0   \n",
       "2450   700  Lame book that only gives generic overviews an...     1.0   \n",
       "2451   700  I did not enjoy reading this book. I expected ...     1.0   \n",
       "2452   700  l started marking all of the recipes l wanted ...     5.0   \n",
       "2453   700                Highly recommend this W30 cookbook!     5.0   \n",
       "\n",
       "             Kategori  \n",
       "0         Best Seller  \n",
       "1         Best Seller  \n",
       "2         Best Seller  \n",
       "3         Best Seller  \n",
       "4         Best Seller  \n",
       "5         Best Seller  \n",
       "6         Best Seller  \n",
       "7         Best Seller  \n",
       "8         Best Seller  \n",
       "9         Best Seller  \n",
       "10        Best Seller  \n",
       "11        Best Seller  \n",
       "12        Best Seller  \n",
       "13        Best Seller  \n",
       "14        Best Seller  \n",
       "15        Best Seller  \n",
       "16        Best Seller  \n",
       "17        Best Seller  \n",
       "18        Best Seller  \n",
       "19        Best Seller  \n",
       "20        Best Seller  \n",
       "21        Best Seller  \n",
       "22        Best Seller  \n",
       "23        Best Seller  \n",
       "24        Best Seller  \n",
       "25        Best Seller  \n",
       "26        Best Seller  \n",
       "27        Best Seller  \n",
       "28        Best Seller  \n",
       "29        Best Seller  \n",
       "...               ...  \n",
       "2424  Non Best Seller  \n",
       "2425  Non Best Seller  \n",
       "2426  Non Best Seller  \n",
       "2427  Non Best Seller  \n",
       "2428  Non Best Seller  \n",
       "2429  Non Best Seller  \n",
       "2430  Non Best Seller  \n",
       "2431  Non Best Seller  \n",
       "2432  Non Best Seller  \n",
       "2433  Non Best Seller  \n",
       "2434  Non Best Seller  \n",
       "2435  Non Best Seller  \n",
       "2436  Non Best Seller  \n",
       "2437  Non Best Seller  \n",
       "2438  Non Best Seller  \n",
       "2439  Non Best Seller  \n",
       "2440  Non Best Seller  \n",
       "2441  Non Best Seller  \n",
       "2442  Non Best Seller  \n",
       "2443  Non Best Seller  \n",
       "2444  Non Best Seller  \n",
       "2445  Non Best Seller  \n",
       "2446  Non Best Seller  \n",
       "2447  Non Best Seller  \n",
       "2448  Non Best Seller  \n",
       "2449  Non Best Seller  \n",
       "2450  Non Best Seller  \n",
       "2451  Non Best Seller  \n",
       "2452  Non Best Seller  \n",
       "2453  Non Best Seller  \n",
       "\n",
       "[2454 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_review_buku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1386, 1068)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best  = len([x for x in data_review_buku['Kategori'] if x == 'Best Seller'])\n",
    "non_best = len([x for x in data_review_buku['Kategori'] if x == 'Non Best Seller'])\n",
    "\n",
    "best, non_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/si/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/si/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/si/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_clean_review_buku = preprocessing_data(data_review_buku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/si/anaconda/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buku</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>disgusted author s shameless attempt use trage...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>familiar tragedy found writing tragedy superfici</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sad see authors book taken personal stories su...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>would write garbag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>highly doubt would slit anyones throat drink b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>sad wanted make quick buck</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>bad read trash father survivor tragedy publish...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>disrespectful soldi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>read one book year on</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>beg youget hands copy book</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best Seller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Buku                                             Review  Rating  \\\n",
       "0     1  disgusted author s shameless attempt use trage...     2.0   \n",
       "1     1   familiar tragedy found writing tragedy superfici     1.0   \n",
       "2     1  sad see authors book taken personal stories su...     1.0   \n",
       "3     2                                 would write garbag     1.0   \n",
       "4     2  highly doubt would slit anyones throat drink b...     1.0   \n",
       "5     2                         sad wanted make quick buck     1.0   \n",
       "6     2  bad read trash father survivor tragedy publish...     1.0   \n",
       "7     2                                disrespectful soldi     1.0   \n",
       "8     3                              read one book year on     5.0   \n",
       "9     3                         beg youget hands copy book     5.0   \n",
       "\n",
       "      Kategori  \n",
       "0  Best Seller  \n",
       "1  Best Seller  \n",
       "2  Best Seller  \n",
       "3  Best Seller  \n",
       "4  Best Seller  \n",
       "5  Best Seller  \n",
       "6  Best Seller  \n",
       "7  Best Seller  \n",
       "8  Best Seller  \n",
       "9  Best Seller  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern=r'[^a-zA-Z]'\n",
    "\n",
    "for i in range(len(data_clean_review_buku)):\n",
    "    data_clean_review_buku['Review'].iloc[i] = re.sub(pattern,' ', data_clean_review_buku['Review'].iloc[i], flags=re.MULTILINE)\n",
    "data_clean_review_buku.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_data = []\n",
    "for index, row in data_review_buku.iterrows():\n",
    "    arr_data.append([row['Buku'],row['Kategori']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extraction = loaded_feature.transform(data_clean_review_buku.Review)\n",
    "predicted = loaded_model.predict_proba(feature_extraction)       \n",
    "score =[]\n",
    "for i in predicted:\n",
    "    score.append(i)\n",
    "\n",
    "sentimen_score = []\n",
    "for i in range(len(score)):\n",
    "    sentimen_score.append(score [i][1])\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2454"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentimen_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sentimen_score)):\n",
    "    data = arr_data[i]\n",
    "    data.append(sentimen_score[i])\n",
    "    arr_data[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_book = ''\n",
    "i = 0\n",
    "for j in range(len(arr_data)):\n",
    "    temp_score = 0.0\n",
    "    if(temp_book == arr_data[j][0]):\n",
    "        i+=1\n",
    "    else:\n",
    "        for k in range(j-i, j):\n",
    "            temp_score +=arr_data[k][2]\n",
    "        if(temp_score > 0.0):\n",
    "            data = arr_data[j-1]\n",
    "            data.append(temp_score/i)\n",
    "            arr_data[j-1] = data\n",
    "        i = 1\n",
    "    if(j == len(arr_data)-1):\n",
    "        for k in range(j-i+1, j+1):\n",
    "            temp_score +=arr_data[k][2]\n",
    "        if(temp_score > 0.0):\n",
    "            data = arr_data[j]\n",
    "            data.append(temp_score/i)\n",
    "            arr_data[j] = data\n",
    "    temp_book = arr_data[j][0]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_combine = [x for x in arr_data if(len(x)>3)]\n",
    "for i in range(len(data_combine)):\n",
    "    data = data_combine[i][:2]\n",
    "    data.append(data_combine[i][3])\n",
    "    data_combine[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(data_combine)\n",
    "label = a[1]\n",
    "score = a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Best Seller', 'Best Seller',\n",
       "       'Best Seller', 'Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller',\n",
       "       'Non Best Seller', 'Non Best Seller', 'Non Best Seller'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.array(label)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/si/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1030: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if pos_label not in present_labels:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label: array(['Best Seller', 'Non Best Seller'],\n      dtype='<U15')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2917a814187d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_sentimen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_sentimen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentimen_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprecision_sentimen_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_sentimen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentimen_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1581\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 108\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1259\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                     raise ValueError(\"pos_label=%r is not a valid label: %r\" %\n\u001b[0;32m-> 1036\u001b[0;31m                                      (pos_label, present_labels))\n\u001b[0m\u001b[1;32m   1037\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pos_label=1 is not a valid label: array(['Best Seller', 'Non Best Seller'],\n      dtype='<U15')"
     ]
    }
   ],
   "source": [
    "sentimen_feature = np.array(score)\n",
    "sentimen_feature = sentimen_feature.reshape(-1, 1)\n",
    "C = 1.0  # SVM regularization parameter\n",
    "model_sentimen = SVC(kernel='linear', C=C)\n",
    "\n",
    "p = cross_val_score(model_sentimen, sentimen_feature, label, cv=10, scoring= 'precision')\n",
    "precision_sentimen_feature = (sum(p)/len(p))\n",
    "r = cross_val_score(model_sentimen, sentimen_feature, label, cv=10, scoring='recall')\n",
    "recall_sentimen_feature = (sum(r)/len(r)) \n",
    "f = cross_val_score(model_sentimen, sentimen_feature, label, cv=10, scoring='f1')\n",
    "f_measure_sentimen_feature = (sum(f)/len(f)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sentimen_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision_sentimen_feature,recall_sentimen_feature,f_measure_sentimen_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "### load data gambar \n",
    "imagePaths = list(paths.list_images(\"data_gambar\"))\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    image = cv2.imread(imagePath)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "    images.append(hsv)\n",
    "    labels.append(label)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "category_encoder = LabelEncoder()\n",
    "y = category_encoder.fit_transform(labels)\n",
    "\n",
    "#### visual feature extraction spp hsv \n",
    "num_channels = len(images[0].shape)\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SpatialPyramidPooling([1, 2, 4, 8, 16, 32], input_shape=(None, None, num_channels)))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "spp_visual_feature = []\n",
    "\n",
    "for index, values in np.ndenumerate(images):\n",
    "    im = np.expand_dims(values, axis=0)\n",
    "    x = model.predict(im)\n",
    "    spp_visual_feature.append(x[0])\n",
    "\n",
    "visual_feature = np.array(spp_visual_feature)\n",
    "\n",
    "\n",
    "# #### Evaluasi model visual\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "model_visual = SVC(kernel='linear', C=C)\n",
    "\n",
    "p = cross_val_score(model_visual, visual_feature, y, cv=10, scoring= 'precision')\n",
    "precision= (sum(p)/len(p))\n",
    "r = cross_val_score(model_visual, visual_feature, y, cv=10, scoring='recall')\n",
    "recall = (sum(r)/len(r)) \n",
    "f = cross_val_score(model_visual, visual_feature, y, cv=10, scoring='f1')\n",
    "f_measure = (sum(f)/len(f)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62028320195014564, 0.6033333333333335, 0.6101218966405304)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision,recall,f_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINE FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1060 is out of bounds for axis 0 with size 1060",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-063c983dab02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcombine_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcombine_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentimen_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcombine_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1060 is out of bounds for axis 0 with size 1060"
     ]
    }
   ],
   "source": [
    "combine_feature = [None]*len(visual_feature)\n",
    "for i in range(len(visual_feature)):\n",
    "    combine_feature[i] = np.hstack((visual_feature[i],sentimen_feature[i]))\n",
    "    \n",
    "combine_feature = np.array(combine_feature)\n",
    "    \n",
    "# # #### Train_model combine\n",
    "C = 1.0  # SVM regularization parameter\n",
    "model_combine = SVC(kernel='linear', C=C)\n",
    "# p = cross_val_score(model_combine, combine_feature, y, cv=10, scoring= 'precision')\n",
    "# precision_combine_feature = (sum(p)/len(p))\n",
    "# r = cross_val_score(model_combine, combine_feature, y, cv=10, scoring='recall')\n",
    "# recall_combine_feature = (sum(r)/len(r)) \n",
    "# f = cross_val_score(model_combine, combine_feature, y, cv=10, scoring='f1')\n",
    "# f_measure_combine_feature = (sum(f)/len(f)) \n",
    "performance_model = []\n",
    "\n",
    "i = 0\n",
    "kf = KFold(n_splits=10)\n",
    "for train, test in kf.split(combine_feature):\n",
    "    X = combine_feature\n",
    "    y = y\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    model_combine_feature = model_combine.fit(X_train,y_train)\n",
    "    report = classification_report(model_combine_feature.predict(X_test),y_test)\n",
    "    print(\"classification_report\", report)\n",
    "    report = report.split()\n",
    "    performance_model.append([report[17],report[18],report[19]])\n",
    "    print(report[17],report[18],report[19])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  precision recall f1-score\n",
       "0      0.65   0.65     0.65\n",
       "1      0.68   0.68     0.68\n",
       "2      0.64   0.63     0.63\n",
       "3      0.63   0.57     0.59\n",
       "4      0.64   0.53     0.56\n",
       "5      0.63   0.61     0.61\n",
       "6      0.60   0.53     0.53\n",
       "7      0.66   0.57     0.60\n",
       "8      0.72   0.71     0.71\n",
       "9      0.67   0.67     0.67"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(performance_model)\n",
    "a.columns = ['precision','recall','f1-score']\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### visual_feature\n",
    "images = []\n",
    "imagePaths = list(paths.list_images(\"prediksi\"))\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    image = cv2.imread(imagePath)\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    images.append(hsv)\n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "num_channels = len(images[0].shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SpatialPyramidPooling([1, 2, 4, 8, 16, 32], input_shape=(None, None, num_channels)))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "spp_visual_feature = []\n",
    "\n",
    "im = np.expand_dims(images[0], axis=0)\n",
    "x = model.predict(im)\n",
    "spp_visual_feature.append(x[0])    \n",
    "\n",
    "visual_feature_prediksi = np.array(spp_visual_feature)\n",
    "\n",
    "### Sentimen_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 179.,  255.,  255., ...,  173.,   31.,  231.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_feature_prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Sentimen_feature \n",
    "data_review_prediksi = pd.read_excel('/home/si/Desktop/TASI-14/prediksi/prediksi.xlsx')\n",
    "data_clean_prediksi = preprocessing_data(data_review_prediksi)\n",
    "feature_prediksi = loaded_feature.transform(data_clean_prediksi.Review)\n",
    "review_prediksi = loaded_model.predict_proba(feature_prediksi)      \n",
    "score =[]\n",
    "for i in review_prediksi:\n",
    "    score.append(i)\n",
    "\n",
    "sentimen_score_prediksi = []\n",
    "for i in range(len(score)):\n",
    "    sentimen_score_prediksi.append(score [i][1])\n",
    "    \n",
    "sentimen_feature_prediksi = np.mean(sentimen_score_prediksi)\n",
    "\n",
    "sentimen_feature_prediksi = sentimen_feature_prediksi.reshape(-1, 1)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename1= 'model_combine 8.pkl '\n",
    "loaded_model_prediksi = pickle.load(open(filename1, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine_feature_prediksi = [None]*len(visual_feature_prediksi)\n",
    "# for i in range(len(visual_feature_prediksi)):\n",
    "combine_feature_prediksi = np.hstack((visual_feature_prediksi,sentimen_feature_prediksi))\n",
    "hasil_prediksi = loaded_model_prediksi.predict(combine_feature_prediksi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=str(hasil_prediksi)\n",
    "if (x == '[1]'):\n",
    "    post='best seller'\n",
    "else:\n",
    "    post='non-best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
